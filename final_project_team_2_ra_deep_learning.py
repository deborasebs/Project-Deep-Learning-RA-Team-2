# -*- coding: utf-8 -*-
"""Final-Project_Team-2_RA_Deep Learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sE7WkhHlcwTMpr1YIzZBiTq6vRa8euPK

# Import Library
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from scipy import stats
from sklearn.preprocessing import MinMaxScaler

# encoder
from sklearn.preprocessing import LabelEncoder

# splitting the data
from sklearn.model_selection import train_test_split

# evaluation metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score

"""# Data Understanding"""

import pandas as pd

# Membaca file CSV dengan enkode ISO-8859-1 (Latin-1)
data = pd.read_csv('/content/190623_rumahcom_tangsel_city_unfiltered.csv', encoding='ISO-8859-1')

# Menampilkan data
data

data.shape #mengetahui ukuran data

data.columns

data.dtypes

data.isnull().sum()

data.isnull().mean() * 100

"""*missing value* terbanyak ada di bath = 67.4%, dan bed= 45.4%"""

missing_values = data.isnull().sum()
subset_columns = ['bed', 'bath', 'listing-floorarea 2']
non_missing_counts = data[subset_columns].count()

print("Missing values:", missing_values[subset_columns])
print("Non missing values:", non_missing_counts)

print(data.duplicated().value_counts())

"""- 24179 : data tidak duplikat
- 5241 data duplikat
"""

print("Jumlah data awal: ",data.shape)

# make a copy of a data frame
df = data.copy()

"""Berdasarkan data understanding informasi diperoleh :
- Data berjumlah 29420 baris dan 7 kolom
- Terdapat missing value pada bed (46.9%), bath (69.6%), dan area price meter atau listing-floor area 2  (12.5%)
- Duplikat data 5241, dan data tidak duplikat 24179
- Type data object dan float .  
"""

# Menghitung koefisien korelasi
pearson_corr = df.corr(method='pearson')
spearman_corr = df.corr(method='spearman')
kendall_corr = df.corr(method='kendall')

# Menggabungkan koefisien korelasi ke dalam satu DataFrame
correlation_df = pd.concat([pearson_corr, spearman_corr, kendall_corr], axis=1, keys=['Pearson', 'Spearman', 'Kendall'])

# Mencetak hasil
print(correlation_df)

"""# Data Preprocessing

Proses yang dilakukan
- Rename kolom
- Menghapus kolom link
- Mengubah tipe data object ke numerik
- Menghapus data duplikat
- Memperbaiki missing value

"""

# Membaca file CSV dengan enkode ISO-8859-1 (Latin-1)
data = pd.read_csv('/content/190623_rumahcom_tangsel_city_unfiltered.csv', encoding='ISO-8859-1')


# Rename columns
df = df.rename(columns={
    'nav-link href': 'link',
    'listing-location': 'location',
    'price': 'price',
    'bed': 'bed',
    'bath': 'bath',
    'listing-floorarea': 'area',
    'listing-floorarea 2': 'area_price_meter'
})

df

df.columns

df.dtypes



df=df.drop(['link'], axis=1)
df

"""### Mengubah Tipe Data Manual + Encoder"""

# Mengubah data menjadi numerik
# Mengubah kolom 'price' menjadi nominal Rupiah dalam angka
df['price'] = df['price'].str.replace(' M', '000000').str.replace(',', '', regex=True).str.replace('[^\d]', '', regex=True).astype(float)

# Menghilangkan " m²" dari kolom 'listing-floorarea'
df['area'] = df['area'].str.replace(' m²', '', regex=True).astype(int)

# Mengubah kolom 'listing-floorarea 2' menjadi nominal Rupiah dalam angka
df['area_price_meter'] = df['area_price_meter'].str.replace('Rp ', '', regex=True).str.replace(',', '', regex=True).str.replace('[^\d]', '', regex=True).astype(float)

# Menampilkan data setelah transformasi
df

# Membuat objek LabelEncoder
label_encoder = LabelEncoder()

# Loop melalui kolom-kolom yang ingin diubah
for column in df.columns:
    if df[column].dtype == 'object':
        df[column] = label_encoder.fit_transform(df[column])

# Menampilkan data setelah transformasi
df

"""### Mengubah Tipe Data Encode"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Membaca file CSV dengan enkode ISO-8859-1 (Latin-1)
data = pd.read_csv('/content/190623_rumahcom_tangsel_city_unfiltered.csv', encoding='ISO-8859-1')
data=data.drop(['nav-link href'], axis=1)

# Membuat objek LabelEncoder
label_encoder = LabelEncoder()

# Loop melalui kolom-kolom yang ingin diubah
for column in data.columns:
    if data[column].dtype == 'object':
        data[column] = label_encoder.fit_transform(data[column])

# Menampilkan data setelah transformasi
data

"""## Percobaan 1"""

# Menghapus Duplikat Data
#df.drop_duplicates(keep='first', inplace=True)
print("Jumlah data terbaru: ", df.shape)
print("Jumlah data awal: ",data.shape)

"""**Missing value:**
- 'bed': 138 missing values
- 'bath': 205 missing values
- 'listing-floorarea 2': 37 missing values

Jumlah total data dalam Tentu, berikut adalah teks tanpa rumus matematika dalam format yang lebih sederhana:

Jumlah total data dalam setiap kolom adalah 29,420.

Karena jumlah missing value relatif kecil, maka biasanya disarankan untuk mengisi missing value daripada menghapusnya.
"""



df.columns

#Memperbaiki missing value

# Menghitung nilai rata-rata (mean) dari kolom
mean_bed = df['bed'].mean()
mean_bath = df['bath'].mean()
mean_area_price_meter = df['area_price_meter'].mean()

# Mengisi missing values dalam kolom dengan nilai mean
df['bed'].fillna(mean_bed, inplace=True)
df['bath'].fillna(mean_bath, inplace=True)
df['area_price_meter'].fillna(mean_area_price_meter, inplace=True)

#Menampilkan mean dari setiap kolom yang akan diisi
print("Mean Bed:", mean_bed)
print("Mean Bath:", mean_bath)
print("Mean area_price_meter:", mean_area_price_meter)

# Menampilkan data setelah mengisi missing values
df

df.isnull().mean() * 100

"""##Exploratory Data Analysis (EDA)"""

df.describe()

#Membuat Boxplot (contoh pada data price)
plt.title("price")
print(sns.boxplot(df['price']))

"""Artinya karena data terlalu besar, hingga box-plot tidak terliha sehingga hanya outliers yang terlihat"""

# Membuat boxplot untuk kolom 'price' dan menampilkan outlier
plt.figure(figsize=(6, 4))
sns.boxplot(x=df['price'], showfliers=True)  # Menampilkan outlier
plt.title("Box Plot of Price (With Outliers)")
plt.ylabel("Price")

plt.show()

# Menghitung Q1, Q3, dan IQR
q1 = df['price'].quantile(0.25)
q3 = df['price'].quantile(0.75)
iqr = q3 - q1

# Menghitung batas atas dan batas bawah untuk outlier
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

# Mengidentifikasi outlier
outliers = df[(data['price'] < lower_bound) | (df['price'] > upper_bound)]

# Menampilkan outlier
print("Outliers in 'price':")
print(outliers['price'])

#buat plot hist untuk masing masing kolom
df.hist(figsize=(10,10))

"""#Multivariate Exploratory Data Analysis"""

# Fungsi Heatmap
corr = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr, vmin=-1, vmax=1, cmap = "coolwarm", annot=True, annot_kws={"size":8}, fmt='.2f', linewidths=0.1, square = True)
plt.title("Korelasi Antar Variabel")
plt.show()

"""Semakin nilai korelasi mendekati 0 menandakan bahwa tidak ada korelasi antara variabel independen dengan price. Sehingga lebih baik dihapus saja."""

# Memilih kolom yang ingin Anda analisis (contoh: 'price')
selected_column = 'price'

# Membuat histogram untuk melihat persebaran data
plt.figure(figsize=(10, 8))
plt.hist(df[selected_column], bins=30, color='skyblue', alpha=0.7)
plt.title(f'Distribution of {selected_column}')
plt.xlabel(selected_column)
plt.ylabel('Frequency')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""# Modelling Data"""

!pip install tensorflow

!pip install torch

import tensorflow as tf
from tensorflow.keras.layers import Bidirectional, LSTM, Dense
from tensorflow.keras.models import Sequential
from keras.layers import SimpleRNN, Dense
from keras.models import Sequential
from keras.optimizers import RMSprop
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

dfn=df

data.columns

"""## Percobaan 1

Layers LSTM dalam mode Bidirectional
Hidden Layer 64, 128, 1
Fungsi aktivasi LSTM = tanh
Fungsi aktivasi lapisan Dense =Relu layer pertama dan sigmoid yang kedua
Optimizer = Adam
Metrik = Accuracy
Epoch = 5
Batch size =
Regularisasi = L2 pada layer LSTM dengan kernel-regularizer=12

"""

df

df=df.drop(['area_price_meter'], axis=1)

df.head()

dfn=df

from sklearn.preprocessing import StandardScaler

# Gabungkan data_normal dan data_attack
X_normal = dfn[['bed', 'bath', 'area']].values
X_attack = dfn[['bed', 'bath', 'area']].values
Y_normal = dfn['price'].values
Y_attack = dfn['price'].values

X = np.concatenate((X_normal, X_attack))
Y = np.concatenate((Y_normal, Y_attack))

# Normalisasi data
scaler = StandardScaler()
X = scaler.fit_transform(X)

for i in range(0,len(Y)):
  if Y[i] =="price":
    Y[i]=0
  else:
    Y[i]=1

features = len(X[0])
samples = X.shape[0]
train_len = 25
input_len = samples - train_len
I = np.zeros((samples - train_len, train_len, features))

for i in range(input_len):
    temp = np.zeros((train_len, features))
    for j in range(i, i + train_len - 1):
        temp[j-i] = X[j]
    I[i] = temp

X.shape





"""# Loss : Binary dan Metriknya : Akurasi"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np
from keras.models import Sequential
from keras.layers import Bidirectional, LSTM, Dense

# Gabungkan data_normal dan data_attack
X_normal = dfn[['bed', 'bath', 'area']].values
X_attack = dfn[['bed', 'bath', 'area']].values
Y_normal = dfn['price'].values
Y_attack = dfn['price'].values

X = np.concatenate((X_normal, X_attack))
Y = np.concatenate((Y_normal, Y_attack))

# Normalisasi data
scaler = StandardScaler()
X = scaler.fit_transform(X)

for i in range(0, len(Y)):
    if Y[i] == "price":
        Y[i] = 0
    else:
        Y[i] = 1

features = len(X[0])
samples = X.shape[0]
train_len = 25
input_len = samples - train_len
I = np.zeros((samples - train_len, train_len, features))

for i in range(input_len):
    temp = np.zeros((train_len, features))
    for j in range(i, i + train_len - 1):
        temp[j - i] = X[j]
    I[i] = temp

X_train, X_test, Y_train, Y_test = train_test_split(I, Y[25:100000], test_size=0.2)

def create_baseline():
    model = Sequential()
    model.add(Bidirectional(LSTM(64, activation='tanh', kernel_regularizer='l2')))  # kernel ini memperkecil ukuran data (reduksi), (min, max, mean)
    model.add(Dense(128, activation='relu', kernel_regularizer='l2'))
    model.add(Dense(1, activation='sigmoid', kernel_regularizer='l2'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

model = create_baseline()
history = model.fit(X_train, Y_train, epochs=10, validation_split=0.2, verbose=1)

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('BRNN Model  Loss Binary')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.savefig('BRNN Model Loss Binary.png')
plt.show()

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])  # Ubah 'acc' ke 'accuracy'
plt.plot(history.history['val_accuracy'])  # Ubah 'val_acc' ke 'val_accuracy'
plt.title('BRNN Model Accuracy Binary')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='lower right')  # Sesuaikan lokasi legenda
plt.savefig('BRNN_Model_Accuracy Binary.png')
plt.show()

!pip install setuptools==65.5.0 "wheel<0.40.0"



"""# Loss : MSE dan Akurasi : MSE"""

from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Bidirectional, LSTM, Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

# Gabungkan data_normal dan data_attack
X_normal = dfn[['bed', 'bath', 'area']].values
X_attack = dfn[['bed', 'bath', 'area']].values
Y_normal = dfn['price'].values
Y_attack = dfn['price'].values

X = np.concatenate((X_normal, X_attack))
Y = np.concatenate((Y_normal, Y_attack))

# Normalisasi data
scaler = StandardScaler()
X = scaler.fit_transform(X)

for i in range(0, len(Y)):
    if Y[i] == "price":
        Y[i] = 0
    else:
        Y[i] = 1

features = len(X[0])
samples = X.shape[0]
train_len = 25
input_len = samples - train_len
I = np.zeros((samples - train_len, train_len, features))

for i in range(input_len):
    temp = np.zeros((train_len, features))
    for j in range(i, i + train_len - 1):
        temp[j - i] = X[j]
    I[i] = temp

X_train, X_test, Y_train, Y_test = train_test_split(I, Y[25:100000], test_size=0.2)

def create_baseline():
    model = Sequential()
    model.add(Bidirectional(LSTM(64, activation='tanh', kernel_regularizer='l2')))
    model.add(Dense(128, activation='relu', kernel_regularizer='l2'))
    model.add(Dense(1, activation='sigmoid', kernel_regularizer='l2'))
    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])
    return model

model = create_baseline()
history1 = model.fit(X_train, Y_train, epochs=10, validation_split=0.2, verbose=1)

import matplotlib.pyplot as plt

# Ambil nilai MSE dari objek history
mse = history.history['mean_squared_error']
val_mse = history.history['val_mean_squared_error']

# Ambil jumlah epoch
epochs = range(1, len(mse) + 1)

# Visualisasi MSE pada setiap epoch
plt.plot(epochs, mse, 'b', label='Training MSE')
plt.plot(epochs, val_mse, 'r', label='Validation MSE')
plt.title('Training and Validation MSE')
plt.xlabel('Epochs')
plt.ylabel('MSE')
plt.legend()
plt.show()

# Ambil nilai loss dari objek history
loss = history.history['loss']
val_loss = history.history['val_loss']

# Ambil jumlah epoch
epochs = range(1, len(loss) + 1)

# Visualisasi Loss pada setiap epoch
plt.plot(epochs, loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""#DTree"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assume X dan Y telah dipersiapkan sebelumnya

# Normalisasi data
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Train-test split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Membuat dan melatih model Decision Tree
dtree_model = DecisionTreeRegressor(random_state=42)
dtree_model.fit(X_train, Y_train)

# Prediksi pada data test
Y_pred_dtree = dtree_model.predict(X_test)

# Evaluasi menggunakan MSE
mse_dtree = mean_squared_error(Y_test, Y_pred_dtree)
print(f'Mean Squared Error (Decision Tree): {mse_dtree}')

"""# Random Forest"""

from sklearn.ensemble import RandomForestRegressor

# Membuat dan melatih model Random Forest
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, Y_train)

# Prediksi pada data test
Y_pred_rf = rf_model.predict(X_test)

# Evaluasi menggunakan MSE
mse_rf = mean_squared_error(Y_test, Y_pred_rf)
print(f'Mean Squared Error (Random Forest): {mse_rf}')

"""# Kesimpulan

## Metrik Binary

## Metrik MSE

Hasil dari MSE
* contonya kita ambil 3 saja
Epoch 1/10
* 1177/1177 [==============================] - 29s 20ms/step - loss: 0.0650 - mean_squared_error: 0.0042 - val_loss: 0.0055 - val_mean_squared_error: 7.8308e-04
* Epoch 2/10
1177/1177 [==============================] - 25s 21ms/step - loss: 0.0043 - mean_squared_error: 6.7281e-04 - val_loss: 0.0033 - val_mean_squared_error: 5.7296e-04
* Epoch 3/10
1177/1177 [==============================] - 22s 18ms/step - loss: 0.0027 - mean_squared_error: 4.9024e-04 - val_loss: 0.0021 - val_mean_squared_error: 4.1492e-04....




Loss dan Mean Squared Error (MSE):

* Loss pada data pelatihan (loss) menurun dari 0.0650 ke 8.2457e-04 selama 10
epoch.
* Mean Squared Error (MSE) pada data pelatihan juga menurun dari 0.0042 ke 1.8668e-04.

Validasi Loss dan Validasi MSE:

* Loss pada data validasi (val_loss) menurun dari 0.0055 ke 6.7919e-04 selama 10 epoch.
* Validasi Mean Squared Error (val_mean_squared_error) pada data validasi juga menurun dari 7.8308e-04 ke 1.5864e-04.


Interpretasi:

* Penurunan nilai loss dan MSE pada kedua data pelatihan dan validasi menunjukkan bahwa model mampu belajar dan terus memperbaiki prediksinya seiring berjalannya waktu.
* Nilai MSE yang lebih rendah menunjukkan bahwa model memiliki tingkat kesalahan yang lebih kecil dalam memprediksi target.
Nilai MSE yang mendekati nol menunjukkan bahwa prediksi model hampir sama dengan nilai sebenarnya.

Performa pada Data Validasi:

* Performa model pada data validasi juga terlihat baik dengan penurunan loss dan MSE yang konsisten.

#Deployment
"""